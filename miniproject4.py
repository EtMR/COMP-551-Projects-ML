# -*- coding: utf-8 -*-
"""Miniproject4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gbEK2ZLuI2xZu-9R8wSMRIGXXFrsbDgZ
"""

import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from random import randint
from string import punctuation

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold 

from keras.models import Model
from keras.models import Sequential

from keras.utils.np_utils import to_categorical
from keras.layers import Concatenate, Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D
from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D

# Read input data
from google.colab import drive
drive.mount('/content/gdrive')  #data is saved in google drive, read the files from google drive
with open('/content/gdrive/My Drive/Colab Notebooks/[COMP 551]/pos.txt', 'r', encoding="latin-1") as f:
    pos_reviews = f.readlines()
with open('/content/gdrive/My Drive/Colab Notebooks/[COMP 551]/neg.txt', 'r', encoding="latin-1") as f:
    neg_reviews = f.readlines()

print('Positive reviews: ', len(pos_reviews), type(pos_reviews))
print('Negative reviews: ', len(neg_reviews), type(neg_reviews))

# Import google pre-trained word2vec model
from gensim.models import KeyedVectors #Copied this from internet
# Creating the model
lookup_table = KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/Colab Notebooks/[COMP 551]/GoogleNews-vectors-negative300.bin', binary=True)

def clean(reviews):
    clean_reviews = []
    for review in reviews:
        clean_review = ''
        for c in review:
            if c not in punctuation:
                clean_review = clean_review + c
            else:
                clean_review = clean_review + ' '
        clean_reviews.append(clean_review)
    return clean_reviews

def vectorize(total_reviews):
    total_reviews = clean(total_reviews)
    xtrains = np.zeros((10662,53,300))
    j = 0
    missing_words = []
    missing_words_count = 0
    total_words_count = 0
    missing_dict = {}
    for i, reviews in enumerate(total_reviews):
        examine = reviews.split()
        for j, word in enumerate(examine):
            try:
                xtrains[i][j] = lookup_table[word]
                total_words_count += 1
            except KeyError:
                if word in list(missing_dict.keys()):
                    xtrains[i][j] = missing_dict[word]
                else:
                    vector = np.random.normal(loc=-0.0036, scale=0.118, size=(300, ))
                    xtrains[i][j] = vector
                    missing_dict[word] = vector
                    missing_words.append(word)
                    missing_words_count += 1
                    total_words_count += 1
    print('missing_words_count', missing_words_count, 'total_words_count', total_words_count)
    return xtrains

total_reviews = pos_reviews + neg_reviews
X = np.float32(vectorize(total_reviews))
X_train = X.reshape(-1, 53, 300, 1)
Y = np.array([1]*len(pos_reviews) + [0]*len(neg_reviews))
Y_train = Y.reshape(-1, 1)
print('X_train', X_train.shape, type(X_train))
print('Y_train', Y_train.shape, type(Y_train))

def TextModel(input_shape):
    
    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!
    X_input = Input(input_shape) # (53, 300, 1)

    # Zero-Padding: pads the border of X_input with zeroes
    X = ZeroPadding2D((1, 1))(X_input) # (55, 302, 1)

    # CONV -> MAXPOOL -> BN -> RELU Block applied to X
    X1 = Conv2D(100, (1, 302), strides = (1, 1), name = 'conv_X1')(X) # (55, 1, 100)
    X1 = MaxPooling2D((55, 1), name='max_pool_X1')(X1) # (1, 1, 100)
    X1 = BatchNormalization(axis = 3, name = 'bn_X1')(X1) # (55, 1, 100) Normalized over filters???
    X1 = Activation('relu')(X1)
    
    # CONV -> MAXPOOL -> BN -> RELU Block applied to X
    X2 = Conv2D(100, (2, 302), strides = (1, 1), name = 'conv_X2')(X) # (54, 1, 100)
    X2 = MaxPooling2D((54, 1), name='max_pool_X2')(X2) # (1, 1, 100)
    X2 = BatchNormalization(axis = 3, name = 'bn_x2')(X2) # (54, 1, 100) Normalized over filters???
    X2 = Activation('relu')(X2)

    # CONV -> MAXPOOL -> BN -> RELU Block applied to X
    X3 = Conv2D(100, (3, 302), strides = (1, 1), name = 'conv_X3')(X) # (53, 1, 100)
    X3 = MaxPooling2D((53, 1), name='max_pool_X3')(X3) # (1, 1, 100)
    X3 = BatchNormalization(axis = 3, name = 'bn_x3')(X3) # (54, 1, 100) Normalized over filters???
    X3 = Activation('relu')(X3)

    
    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED
    X = Concatenate()([X1, X2, X3])
    X = Flatten()(X)
    X = Dropout(0.5)(X)
    X = Dense(32, input_shape=(200,), activation='relu', name='fc0')(X)
    X = Dropout(0.5)(X)
    X = Dense(1, activation='sigmoid', name='fc1')(X)

    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.
    model = Model(inputs = X_input, outputs = X, name='TextModel') 
      
    return model

from keras import optimizers

textModel = TextModel((53, 300, 1))
adam = optimizers.Adam(lr=0.00002)
textModel.compile(loss='binary_crossentropy', optimizer=adam, metrics = ["accuracy"])
history = textModel.fit(X_train, Y_train, validation_split=0.1, epochs=50, batch_size=64, verbose=1)

# Plot training & validation accuracy values
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()